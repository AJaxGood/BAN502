---
output:
  word_document: default
  html_document: default
---
# BAN502 - Predictive Analytics
## Goode, Stephen
### Course Project, PHASE 1
```{r Task 1}
library(tidyverse, quietly = TRUE)
library(gapminder)
library(ggplot2)
library(GGally)
library(MASS)
library(leaps)
library(caret)
library(ROCR)
library(VIM) #visualizing missingness
library(visdat)
library(mice)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(lubridate)
library(cluster) #algorithms for clustering
library(factoextra) #visualization
```

```{r}
rain <- read_csv("rain.csv", 
    col_types = cols(Date = col_date(format = "%m/%d/%Y")))
```

```{r}
str(rain)
summary(rain)
```


```{r Recode levels }
rain = rain %>% mutate(RainTomorrow = as_factor(RainTomorrow)) %>% mutate(RainTomorrow = fct_recode(RainTomorrow,
  "Yes" = "1",
  "No" = "0"))

rain = rain %>% mutate(RainToday= as_factor(RainToday)) %>% mutate(RainToday = fct_recode(RainToday,
  "Yes" = "1",
  "No" = "0"))

rain = rain %>% mutate(WindDir9am = as_factor(WindDir9am)) %>% mutate(WindDir9am = fct_recode(WindDir9am,
  "E" = "1",
  "ENE" = "2",
  "ESE" = "3",
  "N" = "4",
  "NE" = "5",
  "NNE" = "6",
  "NNW" = "7",
  "NW" = "8",
  "S" = "9",
  "SE" = "10",
  "SSE" = "11",
  "SSW" = "12",
  "SW" = "13",
  "W" = "14",
  "WNW" = "15",
  "WSW" = "16"))

rain = rain %>% mutate(WindDir3pm = as_factor(WindDir3pm)) %>% mutate(WindDir3pm = fct_recode(WindDir3pm,
  "E" = "1",
  "ENE" = "2",
  "ESE" = "3",
  "N" = "4",
  "NE" = "5",
  "NNE" = "6",
  "NNW" = "7",
  "NW" = "8",
  "S" = "9",
  "SE" = "10",
  "SSE" = "11",
  "SSW" = "12",
  "SW" = "13",
  "W" = "14",
  "WNW" = "15",
  "WSW" = "16"))

rain = rain %>% mutate(WindGustDir = as_factor(WindGustDir)) %>% mutate(WindGustDir = fct_recode(WindGustDir,
  "E" = "1",
  "ENE" = "2",
  "ESE" = "3",
  "N" = "4",
  "NE" = "5",
  "NNE" = "6",
  "NNW" = "7",
  "NW" = "8",
  "S" = "9",
  "SE" = "10",
  "SSE" = "11",
  "SSW" = "12",
  "SW" = "13",
  "W" = "14",
  "WNW" = "15",
  "WSW" = "16"))

rain = rain %>% mutate(RainToday = as_factor(as.character(RainToday))) %>% mutate(RainToday = fct_recode(RainToday,
  "No" = "0",
  "Yes" = "1"))

rain = rain %>% mutate(RainTomorrow = as_factor(as.character(RainTomorrow))) %>% mutate(RainTomorrow = fct_recode(RainTomorrow,
  "No" = "0",
  "Yes" = "1"))
```

```{r Visualization of Missing Data}
start_time = Sys.time() #for timing

vis_dat(rain)
vis_miss(rain)
vis_miss(rain, cluster = TRUE)

vim_plot = aggr(rain, numbers = TRUE, prop = c(TRUE, FALSE),cex.axis=.7)

end_time = Sys.time()
end_time-start_time
```

```{r Omit or Impute?}
rain_omit = na.omit(rain)

# Dataset went from 28,000+ variables to less than 14,000
```


```{r Imputate Data}
start_time = Sys.time() #for timing
rain_imp = mice(rain, m=1, method = "pmm", seed = 12345)
#in line above: m=1 -> runs one imputation, seed sets the random number seed to get repeatable results
summary(rain_imp)
densityplot(rain_imp)
#red imputed, blue original, only shows density plots when more than 1 value the variable was imputed
#note that the density plots are fairly uninteresting given the small amount of missing data
rain_complete = complete(rain_imp)
summary(rain_complete)
str(rain_complete)
end_time = Sys.time()
end_time-start_time
```

```{r Ensure Data is imputated}
start_time = Sys.time() #for timing

vis_dat(rain_complete)
vis_miss(rain_complete)
vim_plot = aggr(rain_complete, numbers = TRUE, prop = c(TRUE, FALSE),cex.axis=.7)

end_time = Sys.time()
end_time-start_time
```

```{r Remove Date}
rain_complete <- rain_complete[,-1]

```

```{r Correlation Visualization}
ggcorr(rain_complete, label = TRUE) 
ggcorr(rain_complete, label = TRUE)

ggpairs(bike, columns = c("temp", "atemp","hum", "windspeed", "count"))
```



```{r}
ggplot(rain_complete,aes(x=RainTomorrow, y=Temp9am)) + geom_boxplot()
ggplot(rain_complete,aes(x=RainTomorrow, y=Temp3pm)) + geom_boxplot()

ggplot(rain_complete,aes(x=RainTomorrow, y=Humidity9am)) + geom_boxplot()
ggplot(rain_complete,aes(x=RainTomorrow, y=Humidity3pm)) + geom_boxplot()

ggplot(rain_complete,aes(x=RainTomorrow, y=Pressure9am)) + geom_boxplot()
ggplot(rain_complete,aes(x=RainTomorrow, y=Pressure3pm)) + geom_boxplot()

ggplot(rain_complete,aes(x=RainTomorrow, y=Cloud9am)) + geom_boxplot()
ggplot(rain_complete,aes(x=RainTomorrow, y=Cloud3pm)) + geom_boxplot()

ggplot(rain_complete,aes(x=RainTomorrow, y=WindSpeed9am)) + geom_boxplot()
ggplot(rain_complete,aes(x=RainTomorrow, y=WindSpeed3pm)) + geom_boxplot()

ggplot(rain_complete,aes(x=RainTomorrow, y=WindGustSpeed)) + geom_boxplot()

```

```{r}
ggplot(rain_complete,aes(x=WindDir9am, fill = RainTomorrow)) + geom_bar() +
  theme(axis.text.x = element_text(angle=90, vjust = 0.5))
ggplot(rain_complete,aes(x=WindDir3pm, fill = RainTomorrow)) + geom_bar() +
  theme(axis.text.x = element_text(angle=90, vjust=0.5))

ggplot(rain_complete,aes(x=WindGustDir, fill = RainTomorrow)) + geom_bar() +
  theme(axis.text.x = element_text(angle=90, vjust=0.5))
```

```{r Rainfall x Date?}
rain %>%
  mutate(year = lubridate::year(date), 
  month = lubridate::month(date), 
  day = lubridate::day(date))

ggplot(data = rain_complete, aes(Date, Rainfall)) + 
  geom_point()
  theme(axis.text.x = element_text(angle = 90, vjust=0.5, size=5))+
  scale_y_continuous(name="Rainfall", labels = scales::comma)+
  scale_x_continuous(name="Year", breaks=seq())


```

```{r}
# Let's split the data
set.seed(12345)
train.rows = createDataPartition(y = rain_complete$RainTomorrow, p=0.7, list = FALSE) #70% in training
train = rain_complete[train.rows,] 
test = rain_complete[-train.rows,]

```

```{r Full & Empty Mods}
options(scipen = 999) #Disable exponential notation

allmod = glm(RainTomorrow ~., train, family = "binomial") 
summary(allmod)  

emptymod = glm(RainTomorrow ~1, train, family = "binomial")  
summary(emptymod)

```

```{r Backstep}
#backstep
backmod = stepAIC(allmod, direction = "backward", trace = TRUE) 
summary(backmod)
```

```{r}
#forward
forwardmod = stepAIC(emptymod, direction = "forward", scope=list(upper=allmod,lower=emptymod), trace = TRUE) 
summary(forwardmod)
```


```{r ???}
ggplot(train,aes(x=RainToday, fill = RainTomorrow)) + geom_bar()
t1 = table(train$RainTomorrow,train$RainToday)
prop.table(t1, margin = 2)

ggplot(train,aes(x=race, fill = RainTomorrow)) + geom_bar()
t2 = table(train$RainTomorrow,parole$race)
prop.table(t2, margin = 2)
```

```{r}
str(train)
summary(train)
```

```{r Random Forest Model}
fit_control = trainControl(method = "cv",
                           number = 10) #set up 10 fold cross-validation

set.seed(123)
rf_fit = train(x=as.matrix(train[,-20]), y=as.matrix(train$RainTomorrow),    
                method = "ranger",  
                importance = "permutation",
                trControl = fit_control,
               num.trees = 100)
#note the as.matrix command. Passing a tibble to ranger can result in warnings.
```

```{r Trim Data}

```

```{r Task 2}
tree1 = rpart(RainTomorrow ~ Cloud9am + WindSpeed9am + WindDir9am + Pressure9am + Humidity9am, data = train, method="class")
prp(tree1)
fancyRpartPlot(tree1,cex=.5)
prp(tree1)

tree2 = rpart(RainTomorrow ~ Cloud3pm + WindSpeed3pm + WindDir3pm + Pressure3pm + Humidity3pm, data = train, method="class")
fancyRpartPlot(tree2,cex=.5)
prp(tree2)
```

```{r Rainfall x Date?}
rain <- rain %>%
  gather('2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018',key='YR', value='ZHVI')


ggplot(data = rain_complete, aes(Year, RainTomorrow)) + 
  geom_point()
  theme(axis.text.x = element_text(angle = 90, vjust=0.5, size=5))+
  scale_y_continuous(name="Rainfall", labels = scales::comma)+
  scale_x_continuous(name="Year", breaks=seq())


```

```{r}
traincluster = train %>% select("Rainfall","RainTomorrow")
str(traincluster)
```



The classification tree looks odd - there's multiple humidity nodes. Perhaps there's too many variables? I'd like to break our data into two sets - morning and afternoon. Maybe We'll get a cleaner output for the classification tree

```{r Create Morning Dataset}
rain_omit = na.omit(rain)

Morning = dplyr::select(rain_complete, Cloud9am, Pressure9am, Humidity9am, WindGustSpeed, RainTomorrow)

# Let's split the data
set.seed(1234)
train.rows = createDataPartition(y = Morning$RainTomorrow, p=0.7, list = FALSE) #70% in training
train_morning = Morning[train.rows,] 
train_morning = Morning[-train.rows,]
```

```{r Random Forest Model}
start_time = Sys.time() #for timing
fit_control = trainControl(method = "cv",
number = 10) #set up 10 fold cross-validation
set.seed(123)
rf_fit = train(x=as.matrix(Morning[,-5]), y=as.matrix(Morning$RainTomorrow),
method = "ranger",
importance = "permutation",
num.trees = 100,
trControl = fit_control)
end_time = Sys.time()
end_time-start_time
```



```{r varImp}
# Determine most important variable 
varImp(rf_fit)
```
varImp - Removed Temp9am because it was the least important variable  with an Overall score of 0. Replaced Temp9am with WindSpeed9am - no change. WindSpeed9am is 0. Replaced WindSpeed9am with WindGustSpeed


```{r Create Afternoon Dataset}
Afternoon = dplyr::select(rain_complete, WindDir3pm, WindSpeed3pm, Humidity3pm, Pressure3pm, Cloud3pm, Temp3pm, Month, WindGustDir, WindGustSpeed, Rainfall, RainTodayYes, RainTomorrow)

# Let's split the data
set.seed(1234)
train.rows = createDataPartition(y = Afternoon$RainTomorrow, p=0.7, list = FALSE) #70% in training
train_afternoon = Morning[train.rows,] 
train_afternoon = Morning[-train.rows,]
```

```{r Empty & Full mods for New Morning Set}
options(scipen = 999) #Disable exponential notation

allmod1 = glm(RainTomorrow ~., train_morning, family = "binomial") 
summary(allmod1)  

emptymod1 = glm(RainTomorrow ~1, train_morning, family = "binomial")  
summary(emptymod1)

```

```{r Empty & Full mods for New Afternoon Set}
options(scipen = 999) #Disable exponential notation

allmod2 = glm(RainTomorrow ~., train_afternoon, family = "binomial") 
summary(allmod2)  

emptymod2 = glm(RainTomorrow ~1, train_afternoon, family = "binomial")  
summary(emptymod2)

```


```{r Forward Stepwise - MORNING}
#forward
forwardmod1 = stepAIC(emptymod1, direction = "forward", scope=list(upper=allmod1,lower=emptymod1), trace = TRUE) 
summary(forwardmod1)
```

```{r Backstep Stepwise - AFTERNOON}
#backstep
backmod1 = stepAIC(allmod2, direction = "backward", trace = TRUE)
summary(backmod1)
```

Let's see if we can get a better classification tree.  

```{r}
train_morning1 <- dplyr::select(train_morning, -RainToday, -Rainfall)

Migration <- select(UN_migrant, Country, Country_Code, Type, "1990", "1995", "2000", "2005", "2010", "2015")
```


```{r Classification Tree - MORNING}
tree2 = rpart(RainTomorrow ~., train_morning, method = "class")
fancyRpartPlot(tree2)
prp(tree2)
```

```{r Classification Tree - AFTERNOON}
tree3 = rpart(RainTomorrow ~., train_afternoon, method = "class")
fancyRpartPlot(tree3)
prp(tree3)
```

```{r}
ggcorr(Morning, label = TRUE)

ggpairs(train, columns = c("RainTomorrow", "Humidity9am", "Cloud9am", "Temp9am", "Month"))

ggpairs(train, columns = c("RainTomorrow", "Humidity3pm", "Cloud3pm", "Temp3pm", "Month"))
```

```{r}
rain_omit = na.omit(rain)

rain_num = dplyr::select(rain_omit, MinTemp, MaxTemp, Rainfall, WindGustSpeed)

#Afternoon = dplyr::select(rain_complete, Cloud3pm, Pressure3pm, Humidity3pm, WindGustSpeed, RainTomorrow)

rain_scaled = scale(rain_num) 
summary(rain_scaled)
#scale works by calculating the mean and standard deviation of the entire variable, then scales each element by subtracting the mean and dividing by the standard deviation  
```

Perform k-means clustering with a pre-specified number of clusters. I selected 3 clusters. Note that k-means uses randomness, so we need to use set.seed to ensure same clusters.   
```{r}
set.seed(1234)
clusters1 <- kmeans(rain_scaled, 3)
clusters1 #don't do this for very large datasets as the cluster for each row is shown
```

Visualize the clustering  
```{R}
fviz_cluster(clusters1, rain_scaled)
```

Visually identify optimal number of clusters  
```{r}
set.seed(123)
fviz_nbclust(rain_scaled, kmeans, method = "wss") #minimize within-cluster variation
```
Another method  
```{r}
set.seed(123)
fviz_nbclust(rain_scaled, kmeans, method = "silhouette") #maximize how well points sit in their clusters
```

Let's try 5 clusters  
```{r}
set.seed(1234)
clusters2 <- kmeans(customers_scaled, 5)
clusters2 
fviz_cluster(clusters2, customers_scaled)
```
